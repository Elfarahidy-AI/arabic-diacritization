{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LRnbebj1muv0"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrghCaBgiVFz"},"outputs":[],"source":["# !pip install pyarabic\n","# !pip install keras_preprocessing\n","import os\n","import random\n","from enum import Enum\n","import re\n","import numpy as np\n","from pyarabic.araby import separate, tokenize, is_arabicrange, strip_tashkeel, strip_tatweel\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","import tensorflow as tf\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import Sequential,load_model, Model\n","from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, CategoryEncoding, Bidirectional, Input, Concatenate, Dropout, TimeDistributed, RepeatVector, Flatten\n","from keras.initializers import glorot_normal\n","from keras.losses import SparseCategoricalCrossentropy\n","from keras.metrics import SparseCategoricalAccuracy, F1Score\n","from gensim.models import Word2Vec, FastText\n","\n","\n","# import sys\n","# sys.path.append('/content/drive/MyDrive/NLP_Project/')\n","\n","from chars_enums import *\n","from file_reader import FileReader\n","from preprocessor import Preprocessor"]},{"cell_type":"markdown","metadata":{"id":"LipD_UZTAJix"},"source":["### Train Word2Vec Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6JdkoRpAJFD"},"outputs":[],"source":["def train_word2vec():\n","  file_reader = FileReader()\n","  process = Preprocessor()\n","  data = file_reader.open_file(\"train.txt\")\n","  no_tarkeem_data = process.remove_tarkeem(data)\n","  Word2Vec_Train = []\n","  for line in no_tarkeem_data.split('\\n'):\n","      line_no_diacritics = process.remove_diacritics(line)\n","      words = ['<SOS>']\n","      words.extend(tokenize(line_no_diacritics, conditions=is_arabicrange))\n","      words.append('<EOS>')\n","      Word2Vec_Train.append([word.ljust(15, ' ') for word in words])\n","\n","  # Create the Word2Vec model, specifying skip-gram and window size\n","  model = Word2Vec(Word2Vec_Train, sg=1, window=5, vector_size=70)\n","\n","  # Train the model (this might take time depending on dataset size)\n","  model.train(Word2Vec_Train, total_examples=model.corpus_count, epochs=10)\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrljQap-FVqG"},"outputs":[],"source":["### UNCOMMENT THE NEXT LINES TO TRAIN THE WORD2VEC MODEL\n","#word2vec_model = train_word2vec()\n","#word2vec_model.save(\"/content/drive/MyDrive/NLP_Project/word2vec_model/my_word2vec.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Evj24LetHGCW"},"outputs":[],"source":["word2vec_model = Word2Vec.load(\"word2vec_model/Word2Vec.model\")\n","# word2vec_model = Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"7PFE5350iVF8"},"source":["### Model Structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UndMRQtiVF9"},"outputs":[],"source":["def create_model():\n","  with tf.device('/device:GPU:0'):\n","    arabic_chars = 37\n","    num_of_ashkaaal = 16\n","    max_word_length = 15\n","\n","    SelectedLSTM = LSTM\n","\n","    inputs = Input(shape=(max_word_length,))\n","\n","    embeddings = Embedding(input_dim=arabic_chars, output_dim=37)(inputs)\n","\n","    blstm1 = Bidirectional(SelectedLSTM(units=256, return_sequences=True))(embeddings)\n","\n","    output = TimeDistributed(Dense(units=num_of_ashkaaal, activation='softmax'))(blstm1)\n","\n","    model = Model(inputs, output)\n","\n","    model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=[SparseCategoricalAccuracy()])\n","\n","    return model\n","\n","\n","\n","def create_modified_model():\n","  with tf.device('/device:GPU:0'):\n","    word2vec_length = 50\n","\n","    arabic_chars = 37\n","    num_of_ashkaaal = 16\n","    max_word_length = 15\n","\n","    SelectedLSTM = LSTM\n","\n","    input_left = Input(shape=(word2vec_length,))\n","    input_right = Input(shape=(word2vec_length,))\n","    input_word =  Input(shape=(max_word_length,))\n","\n","    word_embedding = Embedding(input_dim=arabic_chars, output_dim=37)(input_word)\n","\n","    repeated_word_LeftEmbedding = RepeatVector(max_word_length)((input_left))\n","    repeated_word_RightEmbedding = RepeatVector(max_word_length)((input_right))\n","\n","    embeddings = Concatenate()([repeated_word_LeftEmbedding, repeated_word_RightEmbedding])\n","\n","    blstm0 = Bidirectional(SelectedLSTM(units=128, return_sequences=True))(embeddings)\n","\n","    concatenated_vector = Concatenate()([word_embedding, blstm0])\n","\n","    blstm1 = Bidirectional(SelectedLSTM(units=256, return_sequences=True))(concatenated_vector)\n","\n","    output = TimeDistributed(Dense(units=num_of_ashkaaal, activation='softmax'))(blstm1)\n","\n","    model = Model([input_left, input_word, input_right], output)\n","\n","    model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=[SparseCategoricalAccuracy()])\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LR_DwOpqqUhA"},"outputs":[],"source":["model = create_modified_model()\n","model.summary()\n","del model"]},{"cell_type":"markdown","metadata":{"id":"I7oUbjfoiVF-"},"source":["## Models Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5alzAOKbiVF_"},"outputs":[],"source":["class UseModel:\n","    def __init__(self,X_train, y_train, epochs, batch_size):\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","\n","    def train(self, model):\n","        history = model.fit(self.X_train, self.y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)#, sample_weight=1 - np.equal(self.X_train, 36))\n","\n","    # def train_modified(self, model):\n","    #     history = model.fit(self.X_train, self.y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=True,\n","    #                         sample_weight = [np.ones((self.X_train[0].shape)),\n","    #                          (np.ones((self.X_train[1].shape)) - np.equal(self.X_train[1], 36)),\n","    #                                          np.ones((self.X_train[2].shape))])\n","\n","    def train_modified(self, model):\n","        history = model.fit(self.X_train, self.y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=True)\n","\n","    def evaluate(self,model):\n","        results = model.evaluate(self.X_train, self.y_train)\n","        model.summary()\n","        print(\"Evaluation Results:\", results[1]*100)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fQ40AZ-GiVF_"},"source":["## Prepare data utilities\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JkbM9QXiVGA"},"outputs":[],"source":["def to_one_hot(ashkal, size):\n","    one_hot = []\n","    for diacritic in ashkal:\n","        coded = [0] * size\n","        if diacritic.encode('utf-8') in ArabicDiacritics_Mapping:\n","            coded[ArabicDiacritics_Mapping[diacritic.encode('utf-8')]] = 1\n","        one_hot.append(coded)\n","\n","    return one_hot\n","\n","def prepare_data(file_name):\n","  file_reader = FileReader()\n","  process = Preprocessor()\n","  data = file_reader.open_file(file_name+\".txt\")\n","  cleaned_data = process.clean_data(data)\n","  no_tarkeem = process.remove_tarkeem(cleaned_data)\n","  process.tokenize_data(no_tarkeem, file_name + \"_words.pickle\", file_name +\"_diacritics.pickle\")\n","  del cleaned_data\n","  del no_tarkeem\n","\n","def prepare_modified_data(file_name):\n","  file_reader = FileReader()\n","  process = Preprocessor()\n","  data = file_reader.open_file(file_name+\".txt\")\n","  no_tarkeem_data = process.remove_tarkeem(data)\n","  all_cleaned_data = []\n","  for line in no_tarkeem_data.split('\\n'):\n","    cleaned_data = [\"<SOS>\"]\n","    cleaned_data.extend(tokenize(line, conditions=is_arabicrange))\n","    cleaned_data.append(\"<EOS>\")\n","    all_cleaned_data.extend(cleaned_data)\n","  process.tokenize_data(\" \".join(all_cleaned_data), file_name + \"_words_mod.pickle\", file_name +\"_diacritics_mod.pickle\")\n","  del cleaned_data\n","  del all_cleaned_data\n","  del no_tarkeem_data\n","\n","def pad_input(lett):\n","  sequences = []\n","  for word in letters_tokens:\n","    if word == \"<SOS>\" or word == \"<EOS>\":\n","      sequences.append(word)\n","      continue\n","    newWord = []\n","    for letter in word:\n","      newWord.append(ArabicCharacters_Mapping[letter.encode('utf-8')])\n","    sequences.append(newWord)\n","\n","  padded_input = pad_sequences(sequences, maxlen=15, padding='post', truncating='post', value=36)\n","  del sequences\n","  return padded_input\n","\n","\n","def pad_output(diacritics_tokens):\n","  output_hot_encoded = []\n","  for ashkaal in diacritics_tokens:\n","      coded = []\n","      for shakl in ashkaal:\n","        coded.append(ArabicDiacritics_Mapping[shakl.encode('utf-8')])\n","      output_hot_encoded.append(coded)\n","\n","  padded_output = pad_sequences(output_hot_encoded, maxlen=15, padding='post', truncating='post', value=15)\n","  del output_hot_encoded\n","  return padded_output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c87loSE7pscM"},"source":["## prepare train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh0u9oNDoz5G"},"outputs":[],"source":["#prepare_data(\"train\")\n","process = Preprocessor()\n","letters_tokens, diacritics_tokens = process.read_tokenized_data(\"train_words.pickle\", \"train_diacritics.pickle\")\n","print(len(letters_tokens))\n","print(len(diacritics_tokens))\n","\n","padded_input = pad_input(letters_tokens)\n","padded_output = pad_output(diacritics_tokens)"]},{"cell_type":"markdown","metadata":{"id":"RJP1GPH_lgrF"},"source":["## Train old Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68v3CIcLldzE"},"outputs":[],"source":["import gc\n","gc.collect()\n","with tf.device('/device:GPU:0'):\n","    X_train = padded_input\n","    y_train = padded_output\n","    epochs = 10\n","    batch_size = 1000\n","    model = create_model()\n","    train_model = UseModel(X_train, y_train, epochs, batch_size)\n","    train_model.train(model)"]},{"cell_type":"markdown","metadata":{"id":"0A0aQwbYko55"},"source":["## save the old model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sw18BBHlzdK-"},"outputs":[],"source":["model.save(\"Models/Model.h5\", save_format='h5')"]},{"cell_type":"markdown","metadata":{"id":"mXC7k5qWK1lP"},"source":["## Training Modified Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRa1Io_WiVGC"},"outputs":[],"source":["import gc\n","gc.collect()\n","with tf.device('/device:GPU:0'):\n","\n","    X_train_L = []\n","    X_train_W = []\n","    X_train_R = []\n","    for i in range (1,len(padded_input)-1):\n","      X_train_L.append(word2vec_model.wv[letters_tokens[i+1]] if letters_tokens[i+1] in word2vec_model.wv else word2vec_model.wv[' '])\n","      X_train_W.append(padded_input[i])\n","      X_train_R.append(word2vec_model.wv[letters_tokens[i-1]] if letters_tokens[i-1] in word2vec_model.wv else word2vec_model.wv[' '])\n","\n","\n","    X_train_L = np.array(X_train_L)\n","    X_train_W = np.array(X_train_W)\n","    X_train_R = np.array(X_train_R)\n","\n","    y_train = padded_output[1:-1]\n","\n","    epochs = 10\n","\n","    batch_size = 1000\n","\n","    model = create_modified_model()\n","    model.summary()\n","\n","    train_model = UseModel([X_train_L, X_train_W, X_train_R], y_train, epochs, batch_size)\n","\n","    train_model.train_modified(model)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vb-5gPyOozB9"},"source":["## Prepare validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7B86jnYozB9"},"outputs":[],"source":["#prepare_data(\"val\")\n","process = Preprocessor()\n","letters_tokens, diacritics_tokens = process.read_tokenized_data(\"val_words.pickle\", \"val_diacritics.pickle\")\n","print(len(letters_tokens))\n","print(len(diacritics_tokens))\n","\n","padded_input = pad_input(letters_tokens)\n","padded_output = pad_output(diacritics_tokens)"]},{"cell_type":"markdown","metadata":{"id":"-yMwTRBChq4f"},"source":["## Evaluate Modified model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srDRKEpBozB9"},"outputs":[],"source":["with tf.device('/device:GPU:0'):\n","    X_train = padded_input\n","\n","    y_train = padded_output\n","\n","    X_train_L = []\n","    X_train_W = []\n","    X_train_R = []\n","    for i in range (1,len(padded_input)-1):\n","      X_train_L.append(word2vec_model.wv[letters_tokens[i+1]] if letters_tokens[i+1] in word2vec_model.wv else word2vec_model.wv[' '])\n","      X_train_W.append(padded_input[i])\n","      X_train_R.append(word2vec_model.wv[letters_tokens[i-1]] if letters_tokens[i-1] in word2vec_model.wv else word2vec_model.wv[' '])\n","\n","\n","    X_train_L = np.array(X_train_L)\n","    X_train_W = np.array(X_train_W)\n","    X_train_R = np.array(X_train_R)\n","\n","    y_train = padded_output[1:-1]\n","\n","    epochs = 10\n","\n","    batch_size = 1000\n","\n","    model_path = \"Models/Modified_Model.h5\"\n","    model = load_model(model_path)\n","    evaluate_model = UseModel([X_train_L, X_train_W, X_train_R], y_train, epochs, batch_size)\n","    evaluate_model.evaluate(model)"]},{"cell_type":"markdown","metadata":{"id":"rudNRYEVhwST"},"source":["## Old Model Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Tdk-QZ3hv8M"},"outputs":[],"source":["import gc\n","gc.collect()\n","with tf.device('/device:GPU:0'):\n","    X_train = padded_input\n","\n","    y_train = padded_output\n","\n","    epochs = 10\n","\n","    batch_size = 1000\n","\n","    model_path = \"Models/Model.h5\"\n","    model = load_model(model_path)\n","    train_model = UseModel(X_train, y_train, epochs, batch_size)\n","\n","    train_model.evaluate(model)\n"]},{"cell_type":"markdown","metadata":{"id":"dhLcwUb8hjg0"},"source":["## Test Old model on one line"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1jCwgqlhHDi"},"outputs":[],"source":["letters = \" فالخيل والليل والبيداء تعرفني والسيف والرمح والقرطاس والقلم\"\n","tokens = tokenize(letters)\n","\n","results = \"\"\n","model_path = \"Models/Model.h5\"\n","model = load_model(model_path)\n","for word in tokens:\n","  if is_arabicrange(word):\n","    newWord = []\n","    for letter in word:\n","      newWord.append(ArabicCharacters_Mapping[letter.encode('utf-8')])\n","    padded_input = pad_sequences([newWord], maxlen=15, padding='post', truncating='post', value=36)\n","    diacritics = model.predict(padded_input)\n","    for j in range(0, len(word)):\n","        results += word[j]\n","        index = np.argmax(diacritics[0][j])\n","        results += ArabicDiacritics_RevMapping[index].decode('utf-8')\n","  else:\n","    results += word\n","  results += \" \"\n","\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"e5h5EhiphZPK"},"source":["## Test Modified Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSMOZ4g1CqJq"},"outputs":[],"source":["letters = \"قال احمد الحق\"\n","tokens = tokenize(letters)\n","\n","results = \"\"\n","\n","\n","model_path = \"Models/Modified_Model.h5\"\n","model = load_model(model_path)\n","for i,word in enumerate(tokens):\n","  if is_arabicrange(word):\n","    newWord = []\n","    for letter in word:\n","      newWord.append(ArabicCharacters_Mapping[letter.encode('utf-8')])\n","    padded_input = pad_sequences([newWord], maxlen=15, padding='post', truncating='post', value=36)\n","\n","    X_train_L = [word2vec_model.wv[letters_tokens[i+1]] if i+1 < len(tokens) and tokens[i+1] in word2vec_model.wv else word2vec_model.wv[' ']]\n","    X_train_W = padded_input\n","    X_train_R = [word2vec_model.wv[letters_tokens[i-1]] if i-1 > 0 and tokens[i-1] in word2vec_model.wv else word2vec_model.wv[' ']]\n","\n","    X_train_L = np.array(X_train_L)\n","    X_train_W = np.array(X_train_W)\n","    X_train_R = np.array(X_train_R)\n","\n","    diacritics = model.predict([X_train_L, X_train_W, X_train_R])\n","    for j in range(0, len(word)):\n","        results += word[j]\n","        index = np.argmax(diacritics[0][j])\n","        results += ArabicDiacritics_RevMapping[index].decode('utf-8')\n","  else:\n","    results += word\n","  results += \" \"\n","\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"5LMGNZxghLw4"},"source":["# uplaod the data in csv seciton"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1704145676609,"user":{"displayName":"Youssef Ismail","userId":"18051074176696177699"},"user_tz":-120},"id":"KroVANnX_rOc","outputId":"9ce85874-0d10-4e67-fab4-16fa44e7c0f5"},"outputs":[],"source":["import pickle\n","\n","\n","file = open(\"dataset/diacritic2id.pickle\",\"rb\")\n","\n","dic = pickle.load(file)\n","\n","print(dic)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25942,"status":"ok","timestamp":1704151694121,"user":{"displayName":"Youssef Ismail","userId":"18051074176696177699"},"user_tz":-120},"id":"qKfP6SV8AWN2","outputId":"896ca544-6f74-4b59-aaaa-96773483a2d8"},"outputs":[],"source":["import csv\n","\n","process = Preprocessor()\n","arabicwords = []\n","\n","model_path = \"Models/Model.h5\"\n","model = load_model(model_path)\n","\n","with open(\"outputs_Model_1.csv\", 'w') as csvfile:\n","  csvwriter = csv.writer(csvfile)\n","\n","  letters = open(\"dataset/test_no_diacritics.txt\", \"r\").readlines()\n","  tokens = tokenize(process.remove_tarkeem(\" \".join(letters)))\n","\n","  i = 0\n","\n","  padded_inputs = []\n","\n","  for word in tokens:\n","    if is_arabicrange(word):\n","      newWord = []\n","      for letter in word:\n","        newWord.append(ArabicCharacters_Mapping[letter.encode('utf-8')])\n","      arabicwords.append(newWord)\n","      padded_input = pad_sequences([newWord], maxlen=15, padding='post', truncating='post', value=36)\n","      padded_inputs.append(padded_input)\n","\n","  padded_inputs = np.array(padded_inputs).reshape(-1, 15)\n","\n","  diacritics = model.predict(padded_inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbnB8tNRU-ng"},"outputs":[],"source":["index = 0\n","output = []\n","\n","with open(\"outputs_Model_1.csv\", 'w') as csvfile:\n","  for i in range(0, len(arabicwords)):\n","    for j in range(0, len(arabicwords[i])):\n","      arindex = np.argmax(diacritics[i][j])\n","      output.append(str(index) + \",\" +  str(dic[ArabicDiacritics_RevMapping[arindex].decode('utf-8')]) + \"\\n\")\n","      index += 1\n","\n","  csvfile.writelines([\"ID,label\\n\"])\n","  csvfile.writelines(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26440,"status":"ok","timestamp":1704150797331,"user":{"displayName":"Youssef Ismail","userId":"18051074176696177699"},"user_tz":-120},"id":"ZjaLKtTm2WNT","outputId":"6c38a72e-3cd2-4e09-df25-b62044b3410e"},"outputs":[],"source":["import csv\n","import pandas\n","\n","process = Preprocessor()\n","arabicwords = []\n","\n","model_path = \"Models/Modified_Model.h5\"\n","model = load_model(model_path)\n","\n","with open(\"outputs_Model_2.csv\", 'w') as csvfile:\n","  csvwriter = csv.writer(csvfile)\n","\n","  letters = open(\"dataset/test_no_diacritics.txt\", \"r\").readlines()\n","  tokens = tokenize(process.remove_tarkeem(\" \".join(letters)))\n","\n","  i = 0\n","\n","X_train_L = []\n","X_train_W = []\n","X_train_R = []\n","\n","for word in tokens:\n","    if is_arabicrange(word):\n","      newWord = []\n","      for letter in word:\n","        newWord.append(ArabicCharacters_Mapping[letter.encode('utf-8')])\n","      arabicwords.append(newWord)\n","      padded_input = pad_sequences([newWord], maxlen=15, padding='post', truncating='post', value=36)\n","\n","      X_train_L_new = [word2vec_model.wv[tokens[i+1]] if i+1 < len(tokens) and tokens[i+1] in word2vec_model.wv else word2vec_model.wv[' ']]\n","      X_train_W_new = padded_input\n","      X_train_R_new = [word2vec_model.wv[tokens[i-1]] if i-1 > 0 and tokens[i-1] in word2vec_model.wv else word2vec_model.wv[' ']]\n","\n","      X_train_L.append(X_train_L_new)\n","      X_train_W.append(X_train_W_new)\n","      X_train_R.append(X_train_R_new)\n","\n","X_train_L = np.array(X_train_L)\n","X_train_W = np.array(X_train_W)\n","X_train_R = np.array(X_train_R)\n","\n","X_train_L = (X_train_L).reshape((-1, 50))\n","X_train_W = (X_train_W).reshape((-1, 15))\n","X_train_R = (X_train_R).reshape((-1, 50))\n","\n","diacritics = model.predict([X_train_L, X_train_W, X_train_R])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKN5h6nAPNcG"},"outputs":[],"source":["index = 0\n","output = []\n","\n","with open(\"outputs_Model_2.csv\", 'w') as csvfile:\n","  for i in range(0, len(arabicwords)):\n","    for j in range(0, len(arabicwords[i])):\n","      arindex = np.argmax(diacritics[i][j])\n","      output.append(str(index) + \",\" +  str(dic[ArabicDiacritics_RevMapping[arindex].decode('utf-8')]) + \"\\n\")\n","      index += 1\n","\n","  csvfile.writelines([\"ID,label\\n\"])\n","  csvfile.writelines(output)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
